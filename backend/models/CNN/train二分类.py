import os
import torch
import torch.nn as nn
import torch.optim as optim
import sys
import torch.nn.functional as F
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..'))
sys.path.append(project_root)

# ä¹Ÿæ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿ç›´æ¥å¯¼å…¥
sys.path.append(current_dir)

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from model_binary import (
        FineGrainedStepClassifier,
        LightweightFineGrainedClassifier,
        StepFeatureExtractor
    )
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    from backend.models.CNN.model_binary import (
        FineGrainedStepClassifier,
        LightweightFineGrainedClassifier,
        StepFeatureExtractor
    )

# === æ•°æ®é›†å’Œæ•°æ®åŠ è½½åŠŸèƒ½ ===

class BinaryAccelerometerDataset(Dataset):
    """äºŒåˆ†ç±»åŠ é€Ÿåº¦æ•°æ®é›†"""

    def __init__(self, data, labels, sequence_length=100):
        self.data = data
        self.labels = labels
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸ºsequence_length
        sample = self.data[idx]
        if len(sample) > self.sequence_length:
            sample = sample[:self.sequence_length]
        elif len(sample) < self.sequence_length:
            # å¦‚æœæ•°æ®ä¸å¤Ÿé•¿ï¼Œè¿›è¡Œå¡«å……
            padding = np.zeros((self.sequence_length - len(sample), sample.shape[1]))
            sample = np.vstack([sample, padding])

        return torch.FloatTensor(sample), torch.LongTensor([self.labels[idx]])

def load_binary_data(data_folder, sequence_length=100, use_feature_enhancement=True):
    """
    åŠ è½½äºŒåˆ†ç±»æ•°æ®ï¼ˆåç€/ç«™ç€ï¼‰

    å‚æ•°:
    - data_folder: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼Œåº”åŒ…å«sitå’Œstandå­æ–‡ä»¶å¤¹
    - sequence_length: åºåˆ—é•¿åº¦
    - use_feature_enhancement: æ˜¯å¦ä½¿ç”¨ç‰¹å¾å¢å¼º

    è¿”å›:
    - features: ç‰¹å¾æ•°æ®
    - labels: æ ‡ç­¾æ•°æ® (0: sit, 1: stand)
    """

    sit_folder = os.path.join(data_folder, "sit")
    stand_folder = os.path.join(data_folder, "stand")

    if not os.path.exists(sit_folder) or not os.path.exists(stand_folder):
        raise ValueError(f"æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {sit_folder} æˆ– {stand_folder}")

    all_features = []
    all_labels = []

    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    if use_feature_enhancement:
        feature_extractor = StepFeatureExtractor()

    # åŠ è½½åç€çš„æ•°æ® (æ ‡ç­¾: 0)
    print("åŠ è½½åç€æ•°æ®...")
    sit_files = glob.glob(os.path.join(sit_folder, "*.csv"))
    for i, file_path in enumerate(sit_files):
        try:
            df = pd.read_csv(file_path)
            if len(df) < 50:  # è·³è¿‡å¤ªçŸ­çš„æ•°æ®
                continue

            # æå–x, y, zåˆ—
            data = df[['x', 'y', 'z']].values

            # ç¡®ä¿æ•°æ®é•¿åº¦
            if len(data) > sequence_length:
                data = data[:sequence_length]
            elif len(data) < sequence_length:
                # å¡«å……æ•°æ®
                padding = np.zeros((sequence_length - len(data), 3))
                data = np.vstack([data, padding])

            # ç‰¹å¾å¢å¼º
            if use_feature_enhancement:
                enhanced_features = feature_extractor.extract_fine_grained_features(data)
                all_features.append(enhanced_features)
            else:
                all_features.append(data)

            all_labels.append(0)  # sit = 0

            if (i + 1) % 100 == 0:
                print(f"å·²åŠ è½½ {i + 1} ä¸ªåç€æ•°æ®æ–‡ä»¶")

        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue

    print(f"åç€æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len([l for l in all_labels if l == 0])} ä¸ªæ ·æœ¬")

    # åŠ è½½ç«™ç€çš„æ•°æ® (æ ‡ç­¾: 1)
    print("åŠ è½½ç«™ç€æ•°æ®...")
    stand_files = glob.glob(os.path.join(stand_folder, "*.csv"))
    for i, file_path in enumerate(stand_files):
        try:
            df = pd.read_csv(file_path)
            if len(df) < 50:  # è·³è¿‡å¤ªçŸ­çš„æ•°æ®
                continue

            # æå–x, y, zåˆ—
            data = df[['x', 'y', 'z']].values

            # ç¡®ä¿æ•°æ®é•¿åº¦
            if len(data) > sequence_length:
                data = data[:sequence_length]
            elif len(data) < sequence_length:
                # å¡«å……æ•°æ®
                padding = np.zeros((sequence_length - len(data), 3))
                data = np.vstack([data, padding])

            # ç‰¹å¾å¢å¼º
            if use_feature_enhancement:
                enhanced_features = feature_extractor.extract_fine_grained_features(data)
                all_features.append(enhanced_features)
            else:
                all_features.append(data)

            all_labels.append(1)  # stand = 1

            if (i + 1) % 100 == 0:
                print(f"å·²åŠ è½½ {i + 1} ä¸ªç«™ç€æ•°æ®æ–‡ä»¶")

        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue

    print(f"ç«™ç€æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len([l for l in all_labels if l == 1])} ä¸ªæ ·æœ¬")

    if len(all_features) == 0:
        print("é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®")
        return None, None

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    features = np.array(all_features)
    labels = np.array(all_labels)

    print(f"æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    print(f"  åç€æ ·æœ¬æ•°: {np.sum(labels == 0)}")
    print(f"  ç«™ç€æ ·æœ¬æ•°: {np.sum(labels == 1)}")

    return features, labels

def create_binary_data_loaders(features, labels, batch_size=32, test_size=0.2, val_size=0.1):
    """
    åˆ›å»ºäºŒåˆ†ç±»æ•°æ®åŠ è½½å™¨

    å‚æ•°:
    - features: ç‰¹å¾æ•°æ®
    - labels: æ ‡ç­¾æ•°æ®
    - batch_size: æ‰¹æ¬¡å¤§å°
    - test_size: æµ‹è¯•é›†æ¯”ä¾‹
    - val_size: éªŒè¯é›†æ¯”ä¾‹

    è¿”å›:
    - train_loader, val_loader, test_loader
    """

    # åˆ†å±‚é‡‡æ ·ç¡®ä¿æ ‡ç­¾åˆ†å¸ƒå¹³è¡¡
    X_temp, X_test, y_temp, y_test = train_test_split(
        features, labels, test_size=test_size, random_state=42, stratify=labels
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size/(1-test_size), random_state=42, stratify=y_temp
    )

    print(f"æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = BinaryAccelerometerDataset(X_train, y_train)
    val_dataset = BinaryAccelerometerDataset(X_val, y_val)
    test_dataset = BinaryAccelerometerDataset(X_test, y_test)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

    return train_loader, val_loader, test_loader

# === æŸå¤±å‡½æ•°ä¼˜åŒ– ===

class FocalLoss(nn.Module):
    """Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œå›°éš¾æ ·æœ¬"""
    def __init__(self, alpha=1, gamma=2, num_classes=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.num_classes = num_classes

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()

class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘ - é˜²æ­¢è¿‡æ‹Ÿåˆ"""
    def __init__(self, num_classes=2, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing

    def forward(self, inputs, targets):
        log_probs = F.log_softmax(inputs, dim=1)
        targets_one_hot = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
        targets_smooth = targets_one_hot * (1 - self.smoothing) + self.smoothing / self.num_classes
        loss = -(targets_smooth * log_probs).sum(dim=1).mean()
        return loss

def train_binary_classifier_advanced(model, train_loader, val_loader, epochs=100, device=None,
                                   model_save_path='best_binary_model.pth', config=None, label2idx=None, idx2label=None):
    """é«˜çº§äºŒåˆ†ç±»å™¨è®­ç»ƒ - ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # è·å–ç±»åˆ«æ•°
    num_classes = config.get('num_classes', 2) if config else 2

    # ç»„åˆæŸå¤±å‡½æ•°
    focal_loss = FocalLoss(alpha=1, gamma=2, num_classes=num_classes).to(device)
    smooth_loss = LabelSmoothingLoss(num_classes=num_classes, smoothing=0.1).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)

    best_val_acc = 0
    patience_counter = 0
    max_patience = 25

    print(f"\nğŸš€ å¼€å§‹é«˜çº§è®­ç»ƒ... (è®¾å¤‡: {device})")
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze().long()
            optimizer.zero_grad()

            outputs = model(batch_x)

            loss1 = focal_loss(outputs, batch_y)
            loss2 = smooth_loss(outputs, batch_y)
            loss = 0.7 * loss1 + 0.3 * loss2

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()

        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze().long()
                outputs = model(batch_x)
                loss = focal_loss(outputs, batch_y)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()

        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total

        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')

        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'config': config,
                'label2idx': label2idx,
                'idx2label': idx2label,
                'best_val_acc': best_val_acc,
                'epoch': epoch
            }
            torch.save(checkpoint, model_save_path)
            print(f"  âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_save_path} (éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.2f}%)")
        else:
            patience_counter += 1
            if patience_counter >= max_patience:
                print(f"  â¹ï¸ æå‰åœæ­¢è®­ç»ƒ (è¿ç»­{max_patience}è½®æ— æ”¹å–„)")
                break

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return best_val_acc

def train_binary_classifier_simple(model, train_loader, val_loader, epochs=50, device=None,
                                 model_save_path='best_binary_model.pth', config=None, label2idx=None, idx2label=None):
    """ç®€å•äºŒåˆ†ç±»å™¨è®­ç»ƒ - ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # ä½¿ç”¨ç®€å•çš„äº¤å‰ç†µæŸå¤±
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    best_val_acc = 0
    patience_counter = 0
    max_patience = 15

    print(f"\nğŸš€ å¼€å§‹ç®€å•è®­ç»ƒ... (è®¾å¤‡: {device})")
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze().long()

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze().long()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()

        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total

        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')

        scheduler.step()

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'config': config or {
                    'model_type': 'lightweight',
                    'num_features': model.features[0].in_channels if hasattr(model, 'features') else 24,
                    'num_classes': 2,
                    'sequence_length': 100,
                    'use_feature_enhancement': True
                },
                'label2idx': label2idx or {"sit": 0, "stand": 1},
                'idx2label': idx2label or {0: "sit", 1: "stand"},
                'best_val_acc': best_val_acc,
                'epoch': epoch
            }
            torch.save(checkpoint, model_save_path)
            print(f"  âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_save_path} (éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.2f}%)")
        else:
            patience_counter += 1
            if patience_counter >= max_patience:
                print(f"  â¹ï¸ æå‰åœæ­¢è®­ç»ƒ (è¿ç»­{max_patience}è½®æ— æ”¹å–„)")
                break

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    return best_val_acc

def main():
    """ä¸»å‡½æ•° - æ”¯æŒç®€å•å’Œé«˜çº§ä¸¤ç§è®­ç»ƒæ¨¡å¼"""
    # --- é…ç½® ---
    DATA_FOLDER = "G:/ä¹Œä¸ƒå…«ç³Ÿçš„ä¸œè¥¿/NUS-Summer-Workshop-AIOT/backend/models/CNN/splited"
    MODEL_TYPE = 'lightweight'  # 'lightweight' æˆ– 'full'
    USE_FEATURE_ENHANCEMENT = True
    SEQUENCE_LENGTH = 100
    BATCH_SIZE = 512  # æ‰¹æ¬¡å¤§å°
    EPOCHS = 100
    TEST_SIZE = 0.2
    VAL_SIZE = 0.1
    MODEL_SAVE_PATH = 'best_binary_model.pth'
    TRAINING_MODE = 'simple'  # 'simple' æˆ– 'advanced'

    print("=== äºŒåˆ†ç±»æ¨¡å‹è®­ç»ƒ (åˆå¹¶ç‰ˆ) ===")
    print(f"æ•°æ®æ–‡ä»¶å¤¹: {DATA_FOLDER}")
    print(f"æ¨¡å‹ç±»å‹: {MODEL_TYPE}")
    print(f"è®­ç»ƒæ¨¡å¼: {TRAINING_MODE}")
    print(f"ä½¿ç”¨ç‰¹å¾å¢å¼º: {USE_FEATURE_ENHANCEMENT}")
    print(f"åºåˆ—é•¿åº¦: {SEQUENCE_LENGTH}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"è®­ç»ƒè½®æ•°: {EPOCHS}")

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤¹
    if not os.path.exists(DATA_FOLDER):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {DATA_FOLDER}")
        return

    sit_folder = os.path.join(DATA_FOLDER, "sit")
    stand_folder = os.path.join(DATA_FOLDER, "stand")

    if not os.path.exists(sit_folder) or not os.path.exists(stand_folder):
        print(f"âŒ é”™è¯¯: sitæˆ–standæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        print(f"sitæ–‡ä»¶å¤¹: {sit_folder} (å­˜åœ¨: {os.path.exists(sit_folder)})")
        print(f"standæ–‡ä»¶å¤¹: {stand_folder} (å­˜åœ¨: {os.path.exists(stand_folder)})")
        return

    # --- æ•°æ®åŠ è½½ ---
    print("\nğŸ“‚ å¼€å§‹åŠ è½½äºŒåˆ†ç±»æ•°æ®...")
    try:
        features, labels = load_binary_data(DATA_FOLDER, SEQUENCE_LENGTH, USE_FEATURE_ENHANCEMENT)

        if features is None:
            print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®")
            return

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {features.shape}, æ ‡ç­¾: {labels.shape}")
        print(f"åç€æ ·æœ¬: {sum(labels == 0)}, ç«™ç€æ ·æœ¬: {sum(labels == 1)}")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # --- åˆ›å»ºæ•°æ®åŠ è½½å™¨ ---
    print("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        train_loader, val_loader, _ = create_binary_data_loaders(
            features, labels,
            batch_size=BATCH_SIZE,
            test_size=TEST_SIZE,
            val_size=VAL_SIZE
        )
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return

    # --- æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ ---
    print(f"\nğŸ¤– åˆ›å»º{MODEL_TYPE}æ¨¡å‹...")
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        num_features = features.shape[-1]
        num_classes = 2  # äºŒåˆ†ç±»

        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"ç‰¹å¾ç»´åº¦: {num_features}")
        print(f"ç±»åˆ«æ•°: {num_classes}")

        if MODEL_TYPE == 'lightweight':
            model = LightweightFineGrainedClassifier(input_size=num_features, num_classes=num_classes)
            print("âœ… ä½¿ç”¨ LightweightFineGrainedClassifier æ¨¡å‹")
        else:
            model = FineGrainedStepClassifier(input_size=num_features, num_classes=num_classes)
            print("âœ… ä½¿ç”¨ FineGrainedStepClassifier æ¨¡å‹")

        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return

    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        'model_type': MODEL_TYPE,
        'num_features': num_features,
        'num_classes': num_classes,
        'sequence_length': SEQUENCE_LENGTH,
        'use_feature_enhancement': USE_FEATURE_ENHANCEMENT
    }

    # æ ‡ç­¾æ˜ å°„
    label2idx = {"sit": 0, "stand": 1}
    idx2label = {0: "sit", 1: "stand"}

    # é€‰æ‹©è®­ç»ƒå‡½æ•°
    print(f"\nğŸš€ å¼€å§‹{TRAINING_MODE}è®­ç»ƒ...")
    try:
        if TRAINING_MODE == 'advanced':
            best_acc = train_binary_classifier_advanced(
                model, train_loader, val_loader,
                epochs=EPOCHS, device=device,
                model_save_path=MODEL_SAVE_PATH,
                config=config,
                label2idx=label2idx,
                idx2label=idx2label
            )
        else:  # simple
            best_acc = train_binary_classifier_simple(
                model, train_loader, val_loader,
                epochs=EPOCHS, device=device,
                model_save_path=MODEL_SAVE_PATH,
                config=config,
                label2idx=label2idx,
                idx2label=idx2label
            )

        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")

        # æç¤ºä¸‹ä¸€æ­¥
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"1. è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æ¨¡å‹: python test_binary_prediction.py")
        print(f"2. ä½¿ç”¨é¢„æµ‹å‡½æ•°è¿›è¡Œæ¨ç†")

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()

